# -*- coding: utf-8 -*-
"""VSB_app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JO2xUOX0dClhafLd3p7fWNofRr6Toxc_
"""


#data structures
import pandas as pd
import pyarrow.parquet as pq
import numpy as np


#used for plotting
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go

#used for feature engineering(signal processing tools)
from scipy.fftpack import fft
from scipy.signal import welch
from siml.sk_utils import *
from siml.signal_analysis_utils import *

from tqdm import tqdm
import ast

import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
#data structures
import pandas as pd
import numpy as np

from keras.layers import *
from keras.models import *
from keras import backend as K # The backend gives us access to tensorflow operations and allow us to create attention class
from keras import optimizers # Allows to access Adam class and modify some parameters
from keras.callbacks import * # This object helps the model to train in a smarter way, avoid overfitting
from keras import activations
from keras import regularizers
from keras import initializers
from keras import constraints
from tensorflow.keras.layers import Attention,LSTM,GRU
from keras.utils.vis_utils import plot_model
import tensorboard
# %load_ext tensorboard
import tensorflow.compat.v1 as tf

# https://stackoverflow.com/a/56569206/4699076
tf.disable_eager_execution()

from sklearn.model_selection import GridSearchCV, StratifiedKFold

import concurrent.futures
import multiprocessing

from flask import Flask, jsonify, request

################################################################################################

# Matthews correlation coefficient calculation used inside Keras model
def matthews_correlation(y_true, y_pred):
  """
  Calculate Matthews Correlation Coefficient.

  References
  ----------
  .. [1] https://en.wikipedia.org/wiki/Matthews_correlation_coefficient
  .. [2] https://www.kaggle.com/tarunpaparaju/vsb-competition-attention-bilstm-with-features/notebook?scriptVersionId=10690570
  """
  y_pred_positive = K.round(K.clip(y_pred, 0, 1))
  y_pred_negative = 1 - y_pred_positive 

  y_positive = K.round(K.clip(y_true, 0, 1))
  y_negative= 1 - y_positive

  tp = K.sum(y_positive * y_pred_positive)
  tn = K.sum(y_negative * y_pred_negative)

  fp = K.sum(y_negative * y_pred_positive)
  fn = K.sum(y_positive * y_pred_negative)

  numerator = (tp * tn - fp * fn)
  denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))

  return numerator / (denominator + K.epsilon())

max_num = 127
min_num = -128

# This function standardize the data from (-128 to 127) to (-1 to 1)
# Theoretically it helps in the NN Model training, but I didn't tested without it
def standardize_data(signal, min_data, max_data, range_needed=(-1,1)):
    if min_data < 0:
        signal_std = (signal + abs(min_data)) / (max_data + abs(min_data))
    else:
        signal_std = (signal - min_data) / (max_data - min_data)
    if range_needed[0] < 0:    
        return signal_std * (range_needed[1] + abs(range_needed[0])) + range_needed[0]
    else:
        return signal_std * (range_needed[1] - range_needed[0]) + range_needed[0]
    
# This is one of the most important peace of code of this Kernel
# Any power line contain 3 phases of 800000 measurements, or 2.4 millions data 
# It would be praticaly impossible to build a NN with an input of that size
# The ideia here is to reduce it each phase to a matrix of <n_dim> bins by n features
# Each bean is a set of 5000 measurements (800000 / 160), so the features are extracted from this 5000 chunk data.
def transform_signal(signal, n_dim=160, min_max=(-1,1)):
    # convert data into -1 to 1
    signal_std = standardize_data(signal, min_data=min_num, max_data=max_num)
    # bucket or chunk size, 5000 in this case (800000 / 160)
    bucket_size = int(800000  / n_dim)
    # new_ts will be the container of the new data
    new_signal = []
    # this for iteract any chunk/bucket until reach the whole sample_size (800000)
    for i in range(0, 800000 , bucket_size):
        # cut each bucket to ts_range
        signal_range = signal_std[i:i + bucket_size]
        # calculate each feature
        mean = signal_range.mean()
        std = signal_range.std() # standard deviation
        std_top = mean + std # I have to test it more, but is is like a band
        std_bot = mean - std
        # I think that the percentiles are very important, it is like a distribuiton analysis from eath chunk
        percentil_calc = np.percentile(signal_range, [0, 1, 25, 50, 75, 99, 100]) 
        max_range = percentil_calc[-1] - percentil_calc[0] # this is the amplitude of the chunk
        relative_percentile = percentil_calc - mean # maybe it could heap to understand the asymmetry
        # now, we just add all the features to new_ts and convert it to np.array
        new_signal.append(np.concatenate([np.asarray([mean, std, std_top, std_bot, max_range]),percentil_calc, relative_percentile]))
    return np.asarray(new_signal)

def data_processing(meta_data,signal_data):
    meta_data=meta_data.drop(['target'], axis = 1)
    meta_data = meta_data.set_index(['signal_id'])
    X_test=[]
    for i in tqdm(signal_data.columns):
        id_measurement, phase = meta_data.loc[int(i)]
        subset_test_col = signal_data[i]
        subset_trans = transform_signal(subset_test_col)
        X_test.append([i, id_measurement, phase, subset_trans])
            
    x_test=np.asarray([np.concatenate([X_test[i][3],X_test[i+1][3], X_test[i+2][3]], axis=1) for i in range(0,len(X_test), 3)])
    return x_test
# https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb
##############################################################################################
class Attention(Layer):
    def __init__(self, step_dim,kernel_initializer=None,
                 kernel_regularizer=None, bias_regularizer=None,
                 kernel_constraint=None, bias_constraint=None,
                 use_bias=True, **kwargs):
        
        self.kernel_initializer = initializers.get('glorot_uniform')
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)

        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint= constraints.get(bias_constraint)

        self.use_bias = use_bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(Attention, self).__init__(**kwargs)
    
    def get_config(self):
        config = super().get_config()
        config['kernel_initializer']=self.kernel_initializer
        config["kernel_regularizer"]=self.kernel_regularizer
        config["bias_regularizer"]=self.bias_regularizer
        config["kernel_constraint"]=self.kernel_constraint

        config["bias_constraint"]=self.bias_constraint 
        config["use_bias"]=self.use_bias
        config["step_dim"]=self.step_dim 
        config["features_dim "]=self.features_dim
        
        return config

    def build(self, input_shape):
        assert len(input_shape) == 3

        self.kernel = self.add_weight(shape=(input_shape[-1],),
                                 name='{}_kernel'.format(self.name),
                                 regularizer=self.kernel_regularizer,
                                 constraint=self.kernel_constraint)
        self.features_dim = input_shape[-1]

        if self.use_bias:
            self.bias = self.add_weight(shape=(input_shape[1],),
                                     initializer='zero',
                                     name='{}_bias'.format(self.name),
                                     regularizer=self.bias_regularizer,
                                     constraint=self.bias_constraint)
        else:
            self.bias = None

        self.built = True


    def call(self, x, mask=None):
        features_dim = self.features_dim
        step_dim = self.step_dim

        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),
                        K.reshape(self.kernel, (features_dim, 1))), (-1, step_dim))

        if self.use_bias:
            eij += self.bias

        eij = K.tanh(eij)

        a = K.exp(eij)

        if mask is not None:
            a *= K.cast(mask, K.floatx())

        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())

        a = K.expand_dims(a)
        weighted_input = x * a
        return K.sum(weighted_input, axis=1)

    def compute_output_shape(self, input_shape):
        return input_shape[0],  self.features_dim



import keras
from keras.models import load_model

from keras.initializers import glorot_uniform
######################################################################################
# https://www.tutorialspoint.com/flask
import flask
app = Flask(__name__)


@app.route('/')
def hello_world():
    return 'Hello World!'

@app.route('/index8050')
def index8050():
    return flask.render_template('index8050.html')


@app.route('/predict', methods=['POST'])

def predict():
    try:
        step_dim=160
    
        max_num = 127
        min_num = -128
        meta_data=pd.read_csv("df_metadata_sample.csv")
        signal_data=pq.read_pandas('sample_signal_data.parquet').to_pandas()
        if (signal_data[(signal_data > 127)].any()).any()==True:
            return jsonify("Error!, Amplitude value is too high")
        elif (signal_data[(signal_data < -128)].any()).any()==True:
            return jsonify("Error!, Amplitude value is too low")
        elif (signal_data.isnull().values.any())==True:
            return jsonify("Error!, Amplitude value is missing")
      
        else:
            
            clf = load_model('model.h5',
                     custom_objects={'Attention': Attention(step_dim),
                                     'GlorotUniform':glorot_uniform
                                     ,'LSTMCell':LSTMCell,
                                  'matthews_correlation':matthews_correlation})
            x_test=data_processing(meta_data,signal_data)
            pred = clf.predict(x_test)
            prediction=[]
            for index,i in enumerate(pred): 
                if i<0.5:
                    prediction.append("Partial discharge is not there for the  signal")
                else:
                    prediction.append("Partial discharge is  there for the signal")
            return jsonify({'prediction': prediction})
    except :
        return jsonify("File name is wrong.Try after proper file renameing")
    
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8050)



